Workshop survey link
========================================================

https://clemson.ca1.qualtrics.com/jfe/form/SV_cAdyi84t74wGwAt


5.ML_SupervisedLearning
========================================================
author:   Tue Vu
date: 

1. Regression
Linear Regression
Multi-Linear Regression (MLR)
Other typical Linear Regression Technique
Logistic Regression
2. Decision Tree
3. Ensemble Prediction
Random Forest
Bagging
Boosting
4. Model based Prediction
Na√Øve Bayes
Linear Discriminant Analysis
5. Regularization & Variable selection
Ridge Regression
LASSO
ELASTIC-NET
6. Dimension Reduction
Principal Component Analysis	
7. Neural Network
8. Support Vector Machine
9. K-Nearest Neighbor
	
5.1.1 Linear Regression
========================================================


```r
library(caret)
data(airquality)

set.seed(123)
#Impute missing value using Bagging approach
PreImputeBag <- preProcess(airquality,method="bagImpute")
airquality_imp <- predict(PreImputeBag,airquality)

indT <- createDataPartition(y=airquality_imp$Ozone,p=0.6,list=FALSE)
training <- airquality_imp[indT,]
testing  <- airquality_imp[-indT,]
```


```r
ggplot(training,aes(Ozone,Temp))+
    geom_point(aes(color=factor(Month),size=factor(Month)))+
    ggtitle("Linear Regression between Ozone and Temperature")
```

Fit a linear model

```r
ModFit <- train(Ozone~Temp,data=training,
                preProcess=c("center","scale"),
                method="lm")
summary(ModFit$finalModel)
```

Prediction

```r
prediction <- predict(ModFit,testing)
cor.test(prediction,testing$Ozone)
postResample(prediction,testing$Ozone)
```

Plot results

```r
output <- cbind(testing$Ozone,prediction)
colnames(output) <- c("obs","sim")
noutput <- as.data.frame(output)
ggplot(noutput,aes(obs,sim))+
    geom_point(color='darkblue',size=4,)+
    geom_smooth(method="lm",se=TRUE)+
    ggtitle("Linear Regression between obs and sim Ozone")
```

5.1.2 Multi-Linear Regression
========================================================


```r
library(GGally)
ggpairs(airquality[,-c(5,6)])
```


```r
modFit2 <- train(Ozone~Solar.R+Wind+Temp,data=training,
                 preProcess=c("center","scale"),
                 method="lm")
summary(modFit2$finalModel)

prediction2 <- predict(modFit2,testing)

cor.test(prediction2,testing$Ozone)
postResample(prediction2,testing$Ozone)

dataout <- data.frame(testing$Ozone,prediction2)
colnames(dataout) <- c("obs","sim")
ggplot(dataout,aes(x=obs,y=sim))+
  geom_point(color='blue')+
  geom_smooth(method=lm,se=TRUE,level=0.9)+
  labs(title="Multi-Linear Regression between Ozone and other aspects")
```

5.1.3 Other type Linear Regression technique
========================================================
- Stepwise Linear Regression

```r
modFit_SLR <- train(Ozone~Solar.R+Wind+Temp,data=training,method="lmStepAIC")
summary(modFit_SLR$finalModel)

prediction_SLR <- predict(modFit_SLR,testing)

cor.test(prediction_SLR,testing$Ozone)
postResample(prediction_SLR,testing$Ozone)
```

- Principal Component Regression

```r
modFit_PCR <- train(Ozone~Solar.R+Wind+Temp,data=training,method="pcr")
summary(modFit_PCR$finalModel)

prediction_PCR <- predict(modFit_PCR,testing)

cor.test(prediction_PCR,testing$Ozone)
postResample(prediction_PCR,testing$Ozone)
```

5.1.4 Logistic Regression
========================================================
- Useful for binary outputs


```r
library(kernlab)
data(spam)
names(spam)

indTrain <- createDataPartition(y=spam$type,p=0.6,list = FALSE)
training <- spam[indTrain,]
testing  <- spam[-indTrain,]

ModFit_glm <- train(type~.,data=training,method="glm")
summary(ModFit_glm$finalModel)
```


```r
predictions <- predict(ModFit_glm,testing)
confusionMatrix(predictions, testing$type)
```

5.2. Decision Tree
========================================================
method="rpart": Recursive Partitioning

Split data

```r
library(caret)
data(iris)
set.seed(123)
indT <- createDataPartition(y=iris$Species,p=0.6,list=FALSE)
training <- iris[indT,]
testing  <- iris[-indT,]
```

How to use splitting?

```r
ModFit_rpart <- train(Species~.,data=training,method="rpart",
                      parms = list(split = "chisquare"))
# gini can be replaced by chisquare, entropy, information

plot(ModFit_rpart$finalModel,uniform = TRUE)
text(ModFit_rpart$finalModel)

#fancier plot
library(rattle)
fancyRpartPlot(ModFit_rpart$finalModel)

predict_rpart <- predict(ModFit_rpart,testing)
confusionMatrix(predict_rpart, testing$Species)

testing$PredRight <- predict_rpart==testing$Species
ggplot(testing,aes(x=Petal.Width,y=Petal.Length))+
  geom_point(aes(col=PredRight))
```

5.3. Ensemble
========================================================
5.3.1. Random Forest
5.3.2. Bagging
5.3.3. Boosting AdaBoost
5.3.4. Boosting - Gradient Boosting Machine

5.3.1 Random Forest
========================================================


```r
ModFit_rf <- train(Species~.,data=training,method="rf",prox=TRUE)

predict_rf <- predict(ModFit_rf,testing)
confusionMatrix(predict_rf, testing$Species)

testing$PredRight <- predict_rf==testing$Species
ggplot(testing,aes(x=Petal.Width,y=Petal.Length))+
  geom_point(aes(col=PredRight))
```

5.3.2 Bagging - Bootstrap Aggregation
========================================================

## What is Bootstrap?


```r
samplemean <- function(x, d) {
  return(mean(x[d]))
}
  
library(boot)
 x <- 1:12
 b = boot(x, samplemean, R=100)  
```

## Bagging


```r
library(ElemStatLearn)
data("ozone")
dim(ozone)
```


```r
dozone <- data.frame(ozone$ozone)
temperature <- ozone$temperature
treebag <- bag(dozone,temperature,B=10,
               bagControl = bagControl(fit=ctreeBag$fit,
                                       pred=ctreeBag$pred,
                                       aggregate=ctreeBag$aggregate))
```


```r
predict_bag1 <- predict(treebag$fits[[1]]$fit,dozone)
predict_bag2 <- predict(treebag$fits[[2]]$fit,dozone)
predict_bag  <- predict(treebag,dozone)
```


```r
p1 <- ggplot(ozone,aes(ozone,temperature))+
      geom_point(color="grey")
p1
p2 <- p1+geom_point(aes(ozone,predict_bag1),color="blue")
p2
p3 <- p2+geom_point(aes(ozone,predict_bag2),color="green")
p3
p4 <- p3+geom_point(aes(ozone,predict_bag),color="red")
p4
```

Bagging for iris

```r
ModFit_bag <- train(as.factor(Species) ~ .,data=training,
                   method="treebag",
                   importance=TRUE)
predict_bag <- predict(ModFit_bag,testing)
confusionMatrix(predict_bag, testing$Species)
plot(varImp(ModFit_bag))
```

5.3.3 Boosting - AdaBoost
========================================================

```r
library(adabag)

ModFit_adaboost <- boosting(Species~.,data=training,mfinal = 10, coeflearn = "Breiman")
importanceplot(ModFit_adaboost)
predict_Ada <- predict(ModFit_adaboost,newdata=testing)
confusionMatrix(testing$Species,as.factor(predict_Ada$class))
```

5.3.4 Boosting - Gradial Boosting Machines
========================================================

```r
ModFit_GBM <- train(Species~.,data=training,method="gbm",verbose=FALSE)
ModFit_GBM$finalModel
predict_GBM <- predict(ModFit_GBM,newdata=testing)
confusionMatrix(testing$Species,predict_GBM)
```

5.4.1 Naive Bayes
========================================================

```r
ModFit_NB <- train(Species~., data=training, method="nb")

predict_NB <- predict(ModFit_NB,testing)
confusionMatrix(testing$Species,predict_NB)
```

5.4.2 Linear Discriminant Analysis
========================================================

```r
ModFit_LDA <- train(Species~., data=training, method="lda")

predict_LDA <- predict(ModFit_LDA,testing)
confusionMatrix(testing$Species,predict_LDA)
```

5.5. Regularization
========================================================

```r
library(caret)
library(ElemStatLearn)
data(prostate)
set.seed(123)
indT <- which(prostate$train==TRUE)
training <- prostate[indT,]
testing  <- prostate[-indT,]

library(PerformanceAnalytics)
chart.Correlation(training[,-10])
```

Predict using Linear Regression

```r
library(coefplot)
ModFit_LR <- lm(lpsa~.,data=prostate)
summary(ModFit_LR)
coefplot(ModFit_LR,intercept=FALSE)
```

Predict using Ridge Regression

```r
library(glmnet)
library(plotmo)
y <- training$lpsa
x <- training[,-c(9,10)]
x <- as.matrix(x)

cvfit_Ridge    <- cv.glmnet(x,y,alpha=0)
plot(cvfit_Ridge)

cvfit_Ridge$lambda.min
cvfit_Ridge$lambda.1se

coef(cvfit_Ridge,s=cvfit_Ridge$lambda.1se)

Fit_Ridge <- glmnet(x,y,alpha=0,standardize = TRUE)
plot_glmnet(Fit_Ridge,label=TRUE,xvar="lambda",
            col=seq(1,8),,grid.col = 'lightgray')

xtest <- testing[,-c(9,10)]
xtest <- as.matrix(xtest)
predict_Ridge <- predict(cvfit_Ridge,newx=xtest,s="lambda.1se")
cor.test(predict_Ridge,testing$lpsa)
postResample(predict_Ridge,testing$lpsa)
```

Predict using LASSO

```r
cvfit_LASSO    <- cv.glmnet(x,y,alpha=1)
plot(cvfit_LASSO)

cvfit_LASSO$lambda.min
cvfit_LASSO$lambda.1se

coef(cvfit_LASSO,s=cvfit_LASSO$lambda.1se)

Fit_LASSO <- glmnet(x,y,alpha=1)
plot_glmnet(Fit_LASSO,label=TRUE,xvar="lambda",
            col=seq(1,8),,grid.col = 'lightgray')

predict_LASSO <- predict(cvfit_LASSO,newx=xtest,s="lambda.1se")
postResample(predict_LASSO,testing$lpsa)

#Using only 3 highest weighted covariates
predict_LASSO3 <- predict(cvfit_LASSO,newx=xtest,s=exp(-1))
postResample(predict_LASSO3,testing$lpsa)
```


5.6. Principal Component Analysis
========================================================

```r
library(PerformanceAnalytics)
data(mtcars)
names(mtcars)
#Ignore vs & am (PCA works good with numeric data )
datain <- mtcars[,c(1:7,10:11)]

chart.Correlation(datain)

cin <- cov(scale(datain))
ein <- eigen(cin)
```


```r
mtcars.pca <- prcomp(datain,center=TRUE,scale=TRUE)
summary(mtcars.pca)
```

Using ggbiplot package:

```r
library(devtools)
install_github("vqv/ggbiplot")
```


```r
library(ggbiplot)
ggbiplot(mtcars.pca)
ggbiplot(mtcars.pca, labels=rownames(mtcars))
ggbiplot(mtcars.pca,ellipse=TRUE,  labels=rownames(mtcars))

mtcars.country <- c(rep("Japan", 3), rep("US",4), rep("Europe", 7),rep("US",3), "Europe", rep("Japan", 3), rep("US",4), rep("Europe", 3), "US", rep("Europe", 3))

ggbiplot(mtcars.pca,ellipse=TRUE,labels=rownames(mtcars),groups = mtcars.country)
```

PCA process using caret packages

```r
data(mtcars)
set.seed(123)
datain <- mtcars[,c(1:7,10:11)]

indT <- createDataPartition(y=datain$mpg,p=0.6,list=FALSE)
training <- datain[indT,]
testing  <- datain[-indT,]

preProc <- preProcess(training[,-1],method="pca",pcaComp = 1)
trainPC <- predict(preProc,training[,-1])
testPC  <- predict(preProc,testing[,-1])

traindat<- cbind(training$mpg,trainPC)
testdat <- cbind(testing$mpg,testPC)

names(traindat) <- c("mpg","PC1")
names(testdat)  <- names(traindat) 

modFitPC<- train(mpg~.,method="lm",data=traindat)

predictand <- predict(modFitPC,testdat)
postResample(testing$mpg,as.vector(predictand))
```

PCR sample

```r
ModFit_PCR <- train(mpg~., data = training, method="pcr")
predict_mpg<- predict(ModFit_PCR,newdata=testing)
postResample(testing$mpg,predict_mpg)
```


5.7. Neural Network
========================================================

```r
library(caret)
library(neuralnet)

datain <- mtcars
set.seed(123)
#Split training/testing
indT <- createDataPartition(y=datain$mpg,p=0.6,list=FALSE)
training <- datain[indT,]
testing  <- datain[-indT,]

#scale the data set
smax <- apply(training,2,max)
smin <- apply(training,2,min)
trainNN <- as.data.frame(scale(training,center=smin,scale=smax-smin))
testNN <- as.data.frame(scale(testing,center=smin,scale=smax-smin))

#Fit Neural Network
set.seed(123)
ModNN <- neuralnet(mpg~cyl+disp+hp+drat+wt+qsec+carb,trainNN, hidden=3,linear.output = T)
plot(ModNN)

#Predict using Neural Network
predictNN <- compute(ModNN,testNN[,c(2:7,11)])
predictmpg<- predictNN$net.result*(smax-smin)[1]+smin[1]
postResample(testing$mpg,predictmpg)
qplot(testing$mpg,predictmpg)
```

5.8. Support Vector Machine
========================================================

```r
library(caret)
data(iris)
set.seed(123)
indT <- createDataPartition(y=iris$Species,p=0.6,list=FALSE)
training <- iris[indT,]
testing  <- iris[-indT,]

ModFit_SVM <- train(Species~.,training,method="svmLinear",preProc=c("center","scale"))

predict_SVM<- predict(ModFit_SVM,newdata=testing)
confusionMatrix(testing$Species,predict_SVM)
```

Using *e1071* package that has better plot:

```r
library(e1071)
Fit_SVM_ln <- svm(Species~Petal.Width+Petal.Length,
               data=training,kernel="sigmoid")
plot(Fit_SVM_ln,training[,3:5])

Fit_SVM_rbg <- svm(Species~Petal.Width+Petal.Length,
               data=training,kernel="radial",gamma=0.1)
plot(Fit_SVM_rbg,training[,3:5])

pred_rbg <- predict(Fit_SVM_ln,testing)
confusionMatrix(testing$Species,pred_rbg)
```

5.9. K-Nearest Neighbour
========================================================

```r
library(caret)
data(iris)
set.seed(123)
indT <- createDataPartition(y=iris$Species,p=0.6,list=FALSE)
training <- iris[indT,]
testing  <- iris[-indT,]

ModFit_KNN <- train(Species~.,training,method="knn",preProc=c("center","scale"),tuneLength=20)

ggplot(ModFit_KNN$results,aes(k,AccuracySD))+
      geom_point(color="blue")+
      labs(title=paste("Optimum K is ",ModFit_KNN$bestTune),
           y="Error")
      
predict_KNN<- predict(ModFit_KNN,newdata=testing)
confusionMatrix(testing$Species,predict_KNN)
```

