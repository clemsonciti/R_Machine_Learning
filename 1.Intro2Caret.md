1.Introduction to Caret

Pre & Post-processing for Machine Learning 
========================================================
author:  Tue Vu - CITI-ACDS 
date:   Feb 2019  

4.1 Data partition: training and testing

4.2 Descriptive statistics

4.3 Preprocessing with missing value

4.4 Preprocessing: transform data

4.5 Visualize important variables

4.6 Feature selection

4.7 Train the model

4.8 Sample Machine Learning Algorithm

4.9 Predict the test set

4.10 Evaluate test result


4.1. Data partition: training & testing
========================================================

In Machine Learning, it is mandatory to have training and testing set. Some time a verification set is also recommended.

In usual practice, a typical sample size of training and testing set are 60% and 40%, respectively


```r
library(caret)
data(iris)
#create seed
set.seed(1234)
#Get row numbers for the training data based on dependent variable
ind <- createDataPartition(y=iris$Species,p=0.6,list=FALSE)
#list=FALSE, prevent returning result as a list
#Create the training set
training_iris <- iris[ind,]
#Create the testing set
testing_iris   <- iris[-ind,]
#Set the input (independent variables) for training set:
X <- training_iris[,1:4]
#Set the input (independent variables) for training set:
Y <- training_iris$Species
```

4.2. Descriptive statistics
========================================================

```r
dim(training)
names(training)
summary(training)
plot(training)
featurePlot(x=training,y=training$Species,
            plot="pairs")
```

First look at data
========================================================

```r
qq <- qplot(Petal.Width,Petal.Length,data=training,col=Species)
qq + geom_smooth(method="lm",formula=y~x)
```
Boxplot

```r
qplot(Species,Petal.Length,data=training,fill=Species,geom=c("boxplot","jitter"))
```


4.3 Preprocessing with missing value
========================================================

### omit NA

```r
library(base)
data("airquality")
new_airquality1 <- na.omit(airquality)
```

### set NA to mean value

```r
NA2mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))
new_airquality2 <-replace(airquality, TRUE, lapply(airquality, NA2mean))
```

### Use Impute to handle missing value

```r
library(RANN)
PreImputeKnn <- preProcess(airquality,method="knnImpute")
PreImputeBag <- preProcess(airquality,method="bagImpute")

DataImputeKnn <- predict(PreImputeKnn,airquality)
DataImputeBag <- predict(PreImputeBag,airquality)
```

4.4 Preprocessing Transform data: Standardization
========================================================

Why standardization?

Some variable has large range, for example: rainfall (0-1000mm), temperature (-10 to 40oC), humidity (0-100%), etc.


```r
ind <- createDataPartition(y=DataImputeBag$Ozone,p=0.6,list=FALSE)
training <- DataImputeBag[ind,]
testing   <- DataImputeBag[-ind,]
```


```r
apply(training,2,mean)
apply(training,2,sd)
```

It is recommended to have the same range for model training.

Using **preProcess** function

```r
PreStd <- preProcess(training[,-c(1,5,6)],method=c("center","scale"))
TrainStd <- predict(PreStd,training[,-c(1,5,6)])
apply(TrainStd,2,mean)
apply(TrainStd,2,sd)

TestStd <- predict(PreStd,testing[,-c(1,5,6)])
apply(TestStd,2,mean)
apply(TestStd,2,sd)
```

Preprocess Standardization in practice

```r
#Model1 <- train()
```

4.4 Preprocessing Transform data: Box-Cox transformation
========================================================

Using **preProcess** function

```r
library(gridExtra)
PreBxCx <- preProcess(training[,-c(5,6)],method="BoxCox")
TrainBxCx <- predict(PreBxCx,training[,-c(5,6)])

plot1 <- ggplot(training,aes(Ozone)) + geom_histogram(bins=30)+labs(title="Original Probability")
plot2 <- ggplot(TrainBxCx,aes(Ozone)) + geom_histogram(bins=30)+labs(title="Box-Cox Transform to Normal")
grid.arrange(plot1,plot2,nrow=2)
```

Impute transformation
========================================================

Using **preProcess** function

```r
library(gridExtra)
PreImpute <- preProcess(training[,-c(5,6)],method="knnImpute")
TraImpute <- predict(PreImpute,training[,-c(5,6)])

plot1 <- ggplot(training,aes(Ozone)) + geom_histogram(bins=30)+labs(title="Original Probability")
plot2 <- ggplot(TraImpute,aes(Ozone)) + geom_histogram(bins=30)+labs(title="Box-Cox Transform to Normal")
grid.arrange(plot1,plot2,nrow=2)
```

4.5 Visualize Important variables
========================================================


```r
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 3))
```

4.6 Train and predict model
========================================================

### Using the most simplest model

```r
modelFit <- train(Ozone~Temp,data=training,method="lm")
modelFit$finalModel

prediction <- predict(modelFit,testing)
```

4.7 Evaluate test result
========================================================

FOr Regression with continuous data

```r
cor(prediction,testing$Ozone)
cor.test(prediction,testing$Ozone)
postResample(prediction,testing$Ozone)
```

For discreet or classification data

```r
ModFit_iris <- train(Species~Petal.Length,data=training_iris,method="rf")
predict_iris  <- predict(ModFit_iris,testing_iris)
confusionMatrix(predict_iris,testing_iris$Species)
twoClassSummary(prediction,testing$Ozone)
```
